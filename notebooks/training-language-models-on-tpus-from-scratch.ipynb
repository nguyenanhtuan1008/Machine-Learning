{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training BERT Language Model From Scratch On TPUs\n",
    "\n",
    "https://www.youtube.com/watch?v=s-3zts7FTDA&t=10s\n",
    "\n",
    "https://www.kaggle.com/abhishek/training-language-models-on-tpus-from-scratch\n",
    "#### In this kernel, I will show you how to train language models, such as BERT, from scratch on TPUs!\n",
    "\n",
    "#### If you like this kernel, consider upvoting it and the associated datasets:\n",
    "\n",
    "- https://www.kaggle.com/abhishek/bert-master\n",
    "- https://www.kaggle.com/abhishek/hindi-oscar-corpus\n",
    "- https://www.kaggle.com/abhishek/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\r\n",
      "  Downloading tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 3.4 MB/s \r\n",
      "\u001b[31mERROR: transformers 2.4.1 has requirement tokenizers==0.0.11, but you'll have tokenizers 0.4.2 which is incompatible.\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.0.11\r\n",
      "    Uninstalling tokenizers-0.0.11:\r\n",
      "      Successfully uninstalled tokenizers-0.0.11\r\n",
      "Successfully installed tokenizers-0.4.2\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -U tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\r\n",
      "  Downloading tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 412.3 MB 27 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.1.0)\r\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.8.1)\r\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.2.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.9.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (3.1.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (3.11.3)\r\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\r\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 35.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.1.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.34.2)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.14.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (0.1.8)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.0.8)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.18.1)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.27.1)\r\n",
      "Collecting tensorflow-estimator==1.15.1\r\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 503 kB 47.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.15) (1.11.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (45.2.0.post20200210)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\r\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.1.0\r\n",
      "    Uninstalling tensorboard-2.1.0:\r\n",
      "      Successfully uninstalled tensorboard-2.1.0\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\r\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.1.0\r\n",
      "    Uninstalling tensorflow-2.1.0:\r\n",
      "      Successfully uninstalled tensorflow-2.1.0\r\n",
      "Successfully installed tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwpt = tokenizers.BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    add_special_tokens=True,\n",
    "    unk_token='[UNK]',\n",
    "    sep_token='[SEP]',\n",
    "    cls_token='[CLS]',\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True,\n",
    "    wordpieces_prefix='##'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwpt.train(\n",
    "    files=[\"../input/hindi-oscar-corpus/hi_dedup_1000.txt\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/hindi-vocab.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwpt.save(\"/kaggle/working/\", \"hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bertsrc\n"
     ]
    }
   ],
   "source": [
    "cd ../input/bertsrc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md\t\t    optimization_test.py\r\n",
      "LICENSE\t\t\t    predicting_movie_reviews_with_bert_on_tf_hub.ipynb\r\n",
      "README.md\t\t    requirements.txt\r\n",
      "__init__.py\t\t    run_classifier.py\r\n",
      "create_pretraining_data.py  run_classifier_with_tfhub.py\r\n",
      "extract_features.py\t    run_pretraining.py\r\n",
      "modeling.py\t\t    run_squad.py\r\n",
      "modeling_test.py\t    sample_text.txt\r\n",
      "multilingual.md\t\t    tokenization.py\r\n",
      "optimization.py\t\t    tokenization_test.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0215 07:16:52.058788 140411286930816 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n",
      "\r\n",
      "W0215 07:16:52.059109 140411286930816 module_wrapper.py:139] From create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n",
      "\r\n",
      "W0215 07:16:52.059427 140411286930816 module_wrapper.py:139] From /kaggle/input/bertsrc/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "W0215 07:16:52.097698 140411286930816 module_wrapper.py:139] From create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\r\n",
      "\r\n",
      "W0215 07:16:52.100157 140411286930816 module_wrapper.py:139] From create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n",
      "\r\n",
      "I0215 07:16:52.100515 140411286930816 create_pretraining_data.py:446] *** Reading from input files ***\r\n",
      "I0215 07:16:52.100694 140411286930816 create_pretraining_data.py:448]   /kaggle/input/hindi-oscar-corpus/hi_dedup_1000.txt\r\n",
      "I0215 07:16:57.521164 140411286930816 create_pretraining_data.py:457] *** Writing to output files ***\r\n",
      "I0215 07:16:57.521463 140411286930816 create_pretraining_data.py:459]   /kaggle/working/tf_examples.tfrecord\r\n",
      "W0215 07:16:57.521808 140411286930816 module_wrapper.py:139] From create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\r\n",
      "\r\n",
      "I0215 07:16:57.523103 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.523416 140411286930816 create_pretraining_data.py:151] tokens: [CLS] । जो बाइक को 9 . 3 हॉ ##र ##सप ##ा ##वर की ताकत [MASK] [MASK] . 9 एन ##एम दखी टॉर ##क परदान करती ह । कपनी न बाइक म सबस बडा बदलाव यही किया ह [MASK] परानी बाइक म [MASK] सी ##सी का इजन [MASK] [MASK] , जिसकी [MASK] म नई [MASK] बहत बहतर ह । [MASK] [MASK] इस बाइक म [SEP] ##ी नदी घा ##टी विकास पराधिकरण क उपाधयकष कीरति सिह [MASK] , परव [MASK] जो ##त सिह ग ##न ##सो [MASK] , परयटन सचिव शल ##श बग ##ोल ##ी , परमख आनद ##ी नगी , वीरदर सिह कड ##ारी , फार ##दीन शख , जिलाधिकारी इ ##दध ##र बौ [MASK] [MASK] एस ##एस ##पी एन ##एस न ##पल [MASK] ##ाल , मरा [SEP]\r\n",
      "I0215 07:16:57.523664 140411286930816 create_pretraining_data.py:161] input_ids: 2 136 441 1076 317 29 18 23 3003 210 1462 209 429 316 3529 3 3 18 29 1568 942 5744 3729 212 1601 1470 128 136 1078 115 1076 120 818 1507 2891 1452 393 128 3 3851 1076 120 3 486 372 319 5207 3 3 16 4602 3 120 847 3 669 2072 128 136 3 3 329 1076 120 4 207 4301 3716 419 1144 6463 97 4894 4482 501 3 16 481 3 441 208 501 99 213 1523 3 16 2445 2394 2324 255 1185 609 207 16 1621 4749 207 3742 16 4768 501 3706 440 16 5311 883 4359 16 6145 86 673 210 2733 3 3 1021 759 508 1568 759 115 2180 3 324 16 751 4\r\n",
      "I0215 07:16:57.523873 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.524095 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.524258 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 15 16 21 38 42 47 48 51 54 59 60 75 78 85 106 113 114 115 123 0\r\n",
      "I0215 07:16:57.524420 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 322 507 319 136 5107 2091 402 2251 1076 1078 115 3742 1708 339 254 2733 2823 16 1594 0\r\n",
      "I0215 07:16:57.524610 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.524742 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.525543 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.525816 140411286930816 create_pretraining_data.py:151] tokens: [CLS] पराना - [MASK] ##ड म जाओ टी वी अशलील वीडियो ऑनलाइन - दख ##ो नि : शलक सबस अचछा वीडियो [MASK] , काम ##क [MASK] po [MASK] ##n movie ##s hd x [MASK] ##ut ##v - a . [MASK] [SEP] b ##o ##y ass n ##ud ##e pi ##c [MASK] ##x ##ico city – the international [MASK] ##es पसद add a la ##y ##er of int ##ri ##gu ##e to me ##x ##ico & ap ##os ; s reg ##ul ##ar se ##as [MASK] fin ##al [MASK] , as we ##e ##k [MASK] will be p ##us ##he ##d b ##ack [MASK] acc ##om [MASK] ##od ##ate the [MASK] ##et [MASK] ##a [MASK] national te [MASK] ##s that will take the fi ##eld in the com [SEP]\r\n",
      "I0215 07:16:57.526046 140411286930816 create_pretraining_data.py:161] input_ids: 2 3048 17 3 220 120 4520 832 531 1703 980 2907 17 525 228 721 30 2990 818 1982 980 3 16 688 212 3 1299 3 246 5004 234 5142 65 3 572 271 17 42 18 3 4 43 251 252 4825 55 3834 242 5167 218 3 261 5792 4177 147 762 4997 3 383 2252 2672 42 5151 252 341 657 5714 446 797 242 787 1148 261 5792 10 559 1064 31 60 3077 648 359 651 532 3 5133 385 3 16 1495 2468 242 260 3 2693 1882 57 737 1468 258 43 6043 3 4170 573 3 1747 493 762 3 604 3 240 3 1170 2122 3 234 3450 2693 4923 762 2932 5755 490 762 909 4\r\n",
      "I0215 07:16:57.526275 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.526468 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.526608 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 3 21 25 27 33 39 41 50 57 59 84 87 93 102 105 109 111 113 116 0\r\n",
      "I0215 07:16:57.526756 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 452 881 16 249 252 909 43 1148 1999 2693 367 242 1079 787 262 1888 4460 657 725 0\r\n",
      "I0215 07:16:57.526921 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.527081 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.527681 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.527938 140411286930816 create_pretraining_data.py:151] tokens: [CLS] विदशी मदरा [MASK] [MASK] करत समय , [MASK] सनिशचित करन क लिए छोटी शरआत [MASK] एक बदधि [MASK] रणनीति ह । एक अचछ [MASK] और बर [MASK] बीच का [MASK] समझना महतवपरण ह । [SEP] व सचिव आर . बी . सिह , पलिस [MASK] ##िरी [MASK] ( अपरा ##ध [MASK] आर . [MASK] . चतरवदी , बोरड आफ टर ##सटी ##ज क उपाधयकष च [MASK] शरी [MASK] हसन जद ##ी साहब , बोरड क सदसय एस . एच . एस . तक कठोर साहब , मौल ##ाना सा ##यम मह ##दी , राजकमार मोहममद [MASK] खान [MASK] आफ महम ##दा ##बाद [MASK] फर ##जान रिज ##वी साहब , मोज ##िज रिज ##वी साहब , महा ##विद [MASK] क परबध ##क अबबास मरत ##जा शम ##सी [SEP]\r\n",
      "I0215 07:16:57.528203 140411286930816 create_pretraining_data.py:161] input_ids: 2 6233 2851 3 3 540 606 16 3 2863 397 97 350 3566 1994 3 338 4639 3 4810 128 136 338 923 3 322 653 3 933 319 3 4782 2633 128 136 4 124 2394 703 18 521 18 501 16 982 3 2036 3 12 3097 232 3 703 18 3 18 2651 16 2321 2697 684 3027 237 97 4894 101 3 740 3 4381 4271 207 2893 16 2321 97 3567 1021 18 2003 18 1021 18 468 4857 2893 16 5888 497 334 647 438 390 16 3443 3914 3 744 3 2697 2581 463 1741 3 824 1670 3316 418 2893 16 3913 747 3316 418 2893 16 1307 2075 3 97 5549 212 3918 5793 518 5361 372 4\r\n",
      "I0215 07:16:57.528409 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.528604 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.528743 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 3 4 8 15 18 24 27 30 45 47 51 54 66 68 84 96 98 103 118 0\r\n",
      "I0215 07:16:57.528893 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 120 1049 2892 563 458 1049 97 1336 2373 4529 13 97 1314 986 418 1383 2893 16 1332 0\r\n",
      "I0215 07:16:57.529049 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.529212 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.529793 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.530048 140411286930816 create_pretraining_data.py:151] tokens: [CLS] की सभी राजनीतिक [MASK] को राजसथान क जाट वरग [MASK] आबादी क मताबिक टिकट [MASK] ##ा चाहिए । [MASK] कहा कि राषटरीय कारय ##कार ##िणी हो बठक 17 अकटबर को जयपर म आयोजित की गई । [MASK] म भारत क सपर ##ण राजयो क [MASK] महासभा क [MASK] पो [MASK] न सब सिह चौधरी को राषटरीय अधयकष क रप म चना । इसस [MASK] सब सिह [SEP] राषटरीय अध ##िव ##शन म माग की गई ह कि [MASK] जाट आरकषण आदोलन क दौरान जाट ##ो पर दरज मकदमा वापस ल । इस अवसर पर कहा गया कि [MASK] सरकार स मजबत [MASK] स [MASK] पर [MASK] रखन और राजसथान चनाव क दौरान सभी राजयो की आबादी क अनसार टिकट दन का परसताव पास किया [MASK] । [SEP]\r\n",
      "I0215 07:16:57.530301 140411286930816 create_pretraining_data.py:161] input_ids: 2 316 642 2260 3 317 3442 97 1761 2217 3 3601 97 2115 3731 3 209 1004 136 3 426 326 1325 649 368 5429 328 3555 1079 3640 317 6126 120 2265 316 544 136 3 120 621 97 1731 243 2819 97 3 3218 97 3 1812 3 115 449 501 1432 317 1325 1427 97 516 120 913 136 1603 3 449 501 4 1325 475 537 448 120 1777 316 544 128 326 3 1761 5864 4008 97 1048 1761 228 312 1773 3597 1781 123 136 329 2412 312 426 413 326 3 498 127 3152 3 127 3 312 3 2387 322 3442 1116 97 1048 642 2819 316 3601 97 2272 3731 571 319 2861 884 393 3 136 4\r\n",
      "I0215 07:16:57.530505 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.530732 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.530963 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 4 10 15 19 26 37 45 48 49 50 64 71 78 91 98 102 104 106 125 0\r\n",
      "I0215 07:16:57.531151 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 4083 316 1858 541 316 3555 1761 655 2034 3820 635 448 6353 136 6353 207 4838 3293 413 0\r\n",
      "I0215 07:16:57.531315 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.531455 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.532018 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.532332 140411286930816 create_pretraining_data.py:151] tokens: [CLS] ##क अशलील ##छ ##िप हए कमर ##छ ##ोट सतन ##जर ##मन ##जा ##पानी पोरन ##जा दराचारी महिलाओ ##तर जडी अशलील ##थाई ##न ##गा ना ##चन ##गन हसत ##ियान ##ाय ##ल ##ॉन ##नि ##पलस ##पत ##नी क साथ सकस ##पर [MASK] ##कव पर ##लो ##भ ##िका [MASK] ##र ##टी ##पो [MASK] ##न क साथ [MASK] ##च ##बधन ##ब ##ड छ ##द ##ब ##दल ##ाब ##हत परान [MASK] ##बाल ##ो वाली ##बत पर ##बरा ##जील [MASK] अशलील ##भारत ##ीय [SEP] [MASK] vidyalaya , [MASK] peo , kinnaur , himachal pradesh ककषा 7 म परवश क [MASK] [MASK] [MASK] भरन हत सामानय दिशा – निरदश kendriya [MASK] [MASK] reckong peo , kinnaur [MASK] himachal pradesh ककषा 8 म परवश क लिए एपलीकशन फॉरम भरन हत सामानय दिशा – [MASK] [SEP]\r\n",
      "I0215 07:16:57.532564 140411286930816 create_pretraining_data.py:161] input_ids: 2 212 1703 281 569 624 1849 281 718 5382 627 633 518 5513 5309 518 6421 2095 363 2714 1703 5739 213 360 730 1739 899 3787 1411 399 224 1107 2034 2342 585 403 97 386 881 407 3 5400 312 506 225 628 3 210 419 3842 3 213 97 386 3 227 2401 229 220 102 214 229 1737 2032 1742 1676 3 3018 228 1070 1928 312 3354 3427 3 1703 3592 2155 4 3 806 16 3 772 16 804 16 803 622 683 27 120 763 97 3 3 3 1866 695 1434 1050 147 868 805 3 3 802 772 16 804 3 803 622 683 28 120 763 97 350 1987 1813 1866 695 1434 1050 147 3 4\r\n",
      "I0215 07:16:57.532770 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.532968 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.533136 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 16 19 26 40 46 50 54 61 66 74 79 82 94 95 96 104 105 110 126 0\r\n",
      "I0215 07:16:57.533311 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 5513 391 899 569 726 210 970 229 2095 97 805 802 350 1987 1813 806 16 16 868 0\r\n",
      "I0215 07:16:57.533471 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.533621 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.534229 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.534484 140411286930816 create_pretraining_data.py:151] tokens: [CLS] ##ला रग [MASK] [MASK] ह [MASK] रग मॉडल क # 6b6225 म 41 [MASK] 96 % लाल 38 . 43 % हरा और 14 . 51 % नीला होता [MASK] । hsl वरण [MASK] [MASK] # 6b6225 म 52 डिगरी का रग , 49 % सतपति दवीप 28 % हलकापन होता ह . इस रग की अनमानित तरग दरधय 5 ##7 ##3 . [SEP] ##ading . . . . सामानय को ##टा ततकाल परी ##मि . ततकाल विदशी टरिसट [MASK] [MASK] ##न ##स महिला निचल ##ी बरथ यवा विक ##लाग ड ##य ##टी पास [MASK] ##लिया ##मट शरणी 1 ##a - परथम वातानकलित 2 ##a - दवितीय वातानकलित [MASK] ##a [MASK] ततीय वातानकलित c ##c - [MASK] करसी [MASK] f [MASK] - परथम शरणी sl - [SEP]\r\n",
      "I0215 07:16:57.534714 140411286930816 create_pretraining_data.py:161] input_ids: 2 339 672 3 3 128 3 672 4979 97 7 6377 120 3648 3 3653 9 1104 5106 18 2667 9 2514 322 727 18 3224 9 2592 701 3 136 4190 3458 3 3 7 6377 120 4158 3734 319 672 16 5107 9 4729 1491 1334 9 5017 701 128 18 329 672 316 5020 4585 4591 25 282 274 18 4 3909 18 18 18 18 1434 317 538 4121 634 3335 18 4121 6233 5857 3 3 213 238 919 4711 207 3500 1579 568 5594 108 215 419 884 3 1458 1194 1953 21 240 17 2349 5103 22 240 17 3511 5103 3 240 3 5279 5103 44 218 17 3 3854 3 47 3 17 2349 1953 2940 17 4\r\n",
      "I0215 07:16:57.534916 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.535139 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.535291 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 3 4 6 14 30 34 35 47 80 81 91 95 100 109 111 117 119 121 125 0\r\n",
      "I0215 07:16:57.535441 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 319 701 5040 18 128 1074 97 322 2965 223 108 598 240 23 17 5103 1091 218 2940 0\r\n",
      "I0215 07:16:57.535597 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.535735 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.536306 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.536565 140411286930816 create_pretraining_data.py:151] tokens: [CLS] ##पिग [MASK] ऑनलाइन [MASK] ##मट करन क लिए अपन कारड का इसतमाल करत ह तो यह आपशन [MASK] लिए ह | [MASK] [MASK] आपशन [MASK] off करक आप अपन डबिट कारड का ऑनलाइन mis [MASK] ##e होन स [MASK] एकसप ह | वस दोसतो [MASK] pos machine को बड - बड माल ##स , दकानो अ [MASK] ##et [MASK] ##l p ##um ##ps , आदि म लगा दखा होगा | जिसम आपको अपना कारड सवा ##इ ##प करक पि ##न नबर डाल ##ना [SEP] अगर आपको अपन डबिट [MASK] को [MASK] समय क लिए बद [MASK] ह [MASK] तो आप इस आपशन का इसतमाल कर सकत ह | इस आपशन को off करन स आपका एटीएम कारड किसी भी atm machine ३ काम नही करगा | [SEP]\r\n",
      "I0215 07:16:57.536805 140411286930816 create_pretraining_data.py:161] input_ids: 2 3374 3 2907 3 1194 397 97 350 369 1069 319 1147 540 128 377 416 1947 3 350 128 69 3 3 1947 3 1372 1014 353 369 3580 1069 319 2907 5160 3 242 558 127 3 3058 128 69 1453 1293 3 3678 3485 317 1007 17 1007 1056 238 16 6164 84 3 604 3 247 57 2043 4462 16 1337 120 583 1962 837 69 1113 681 783 1069 826 235 259 1014 1346 213 1575 1902 320 4 552 681 369 3580 3 317 3 606 97 350 581 3 128 3 377 353 329 1947 319 1147 315 564 128 69 329 1947 317 1372 397 127 1142 2895 1069 536 348 1998 3485 141 688 357 1321 69 4\r\n",
      "I0215 07:16:57.537011 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.537228 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.537372 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 2 4 18 22 23 25 35 39 40 45 56 57 59 68 89 91 96 98 122 0\r\n",
      "I0215 07:16:57.537538 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 545 116 932 966 329 317 737 1815 564 2477 16 57 1315 1962 1069 494 563 16 312 0\r\n",
      "I0215 07:16:57.537690 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.537825 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.538398 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.538661 140411286930816 create_pretraining_data.py:151] tokens: [CLS] ह तो जगह को ना ##प लीजिए । [MASK] हिसस [MASK] गारडन [MASK] ? कितना पा ##थ क लिए छोड ##ना ह ? आदि । अब सी ##जन क [MASK] स अपन मन पसद फल चन लीजिए । गारडन को अपनी इचछा क अनसार 2 या 3 भाग ##ो [MASK] बाट दी ##जिए । अब इन भाग म अलग - अलग तरह क [SEP] . 18 अकटबर [MASK] पद ##ा हए हिदी सिनमा क [MASK] कलाकार [MASK] परी का जीवन [MASK] [MASK] [MASK] भरा रहा ह । हालाकि परद पर उनहोन [MASK] तरह क किरदार [MASK] जी ##वत कर दिया । [MASK] स अधिक फिलमो म काम करन और हर [MASK] म बिना किसी ऑ [MASK] ##िशन क रो ##ल पान वाल ओम परी कभी एकटर [SEP]\r\n",
      "I0215 07:16:57.538891 140411286930816 create_pretraining_data.py:161] input_ids: 2 128 377 1481 317 730 259 4706 136 3 3547 3 2650 3 35 3175 394 239 97 350 1387 320 128 35 1337 136 496 486 590 97 3 127 369 434 2252 1347 925 4706 136 2650 317 578 3184 97 2272 22 545 23 1957 228 3 3887 512 2769 136 496 658 1957 120 935 17 935 738 97 4 18 908 3640 3 2017 209 624 1631 967 97 3 3550 3 634 319 830 3 3 3 4322 542 128 136 1713 4472 312 541 3 738 97 6231 3 453 1159 315 459 136 3 127 697 2393 120 688 397 322 623 3 120 1786 536 94 3 2577 97 664 224 2018 462 1176 634 1177 2359 4\r\n",
      "I0215 07:16:57.539106 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.539315 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.539467 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 5 9 11 13 30 50 57 68 75 77 81 82 83 92 95 96 102 111 116 0\r\n",
      "I0215 07:16:57.539619 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 730 2055 120 3883 6005 120 1957 317 5011 1176 2362 228 127 623 6231 317 879 650 220 0\r\n",
      "I0215 07:16:57.539778 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.539913 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.540493 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.540756 140411286930816 create_pretraining_data.py:151] tokens: [CLS] अज ##ञात 2 बाइक सवार बदमाशो [MASK] अचानक गाडी म सवार बीजपी नता पर अध ##ाध [MASK] फायरिग करनी शर कर दी । [MASK] तक गाडी म सवार लोग कछ समझ पात बदमाश वहा स फरार हो गए [MASK] इस हमल म बीजपी नता [MASK] पर म [MASK] [MASK] ह । [MASK] क साथ ##ियो न तरत इसकी सचना पलिस को दी । [MASK] नता को इलाज क लिए असपताल म भरती कराया [MASK] मौक [MASK] पहची पलिस न घटना ##सथल का जाय ##जा [MASK] अज ##ञात लोगो क खिलाफ [MASK] दरज कर लिया ह । [MASK] [SEP] मिथ ##न राशि वालो आज आपका पड ##िग काम परा होगा । [MASK] की सी ##ढ [MASK] आप [MASK] चढ ##त रहग । . . . read more [SEP]\r\n",
      "I0215 07:16:57.540981 140411286930816 create_pretraining_data.py:161] input_ids: 2 1173 5543 22 1076 2509 4124 3 2649 4752 120 2509 1435 1242 312 475 1734 3 6269 5650 428 315 512 136 3 468 4752 120 2509 978 494 1110 3074 3479 1156 127 4737 328 638 3 329 3425 120 1435 1242 3 312 120 3 3 128 136 3 97 386 405 115 2820 1323 1622 982 317 512 136 3 1242 317 3552 97 350 6058 120 3125 3393 3 2857 3 2397 982 115 1291 5917 319 1255 518 3 1173 5543 663 97 1209 3 1773 315 955 128 136 3 4 2739 213 1682 2210 502 1142 848 433 688 548 837 136 3 316 486 256 3 353 3 2305 208 4547 136 18 18 18 1851 1887 4\r\n",
      "I0215 07:16:57.541205 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.541422 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.541567 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 7 17 24 34 37 39 45 48 49 52 64 74 76 85 91 97 111 115 117 0\r\n",
      "I0215 07:16:57.541718 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 115 232 504 1156 328 136 97 1978 1819 1242 850 136 312 768 1870 982 2892 321 2082 0\r\n",
      "I0215 07:16:57.541978 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.542132 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.542679 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.542936 140411286930816 create_pretraining_data.py:151] tokens: [CLS] पास ki [MASK] ##n ##9 ##80 soc [MASK] [MASK] जो ##कि 7 ##nm पर अा ##धार ##ित ह । वही छोट डिजाइन होन क साथ य [MASK] बाकी क परोससर ##स की तलना म काफी बहतर काम करता ह । उदा ##हर ##ण क लिए ki ##ri ##n 9 ##80 soc राबडी 6 . 9 बिल ##ियन [MASK] ##ा ##ज ##िसटर [MASK] 20 [SEP] काफी सधार होगा और [MASK] को इसस बटरी की ma ##h [MASK] को बढान [MASK] कोई [MASK] [MASK] रहगी । वही [MASK] पहल iphone 8 , iphone 8 plus और [MASK] x म दिए गए a 11 परोससर स बटरी परफॉरमस म सधार टिप ##अ ##ा ह । [MASK] एपपल की सबस बडी चिप निरमाता कपनी t ##s [MASK] ##c न अपनी [SEP]\r\n",
      "I0215 07:16:57.543197 140411286930816 create_pretraining_data.py:161] input_ids: 2 884 2119 3 246 244 2780 2688 3 3 441 500 27 5489 312 4216 584 342 128 136 850 2489 1873 558 97 386 121 3 2806 97 2092 238 316 2251 120 770 2072 688 814 128 136 5213 615 243 97 350 2119 446 246 29 2780 2688 4515 26 18 29 1909 1853 3 209 237 4523 3 333 4 770 1732 837 322 3 317 1603 2248 316 597 264 3 317 5882 3 527 3 3 3884 136 850 3 635 2437 28 16 2437 28 4867 322 3 65 120 1342 638 42 743 2092 127 2248 4936 120 1732 2014 276 209 128 136 3 6038 316 818 1100 2307 6282 1078 61 234 3 218 115 578 4\r\n",
      "I0215 07:16:57.543404 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.543601 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.543742 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 3 8 9 27 52 54 58 62 63 69 76 79 81 82 86 95 108 113 123 0\r\n",
      "I0215 07:16:57.543891 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 446 2092 128 2092 2092 18 684 322 333 1078 4647 316 4267 357 1603 2437 128 850 262 0\r\n",
      "I0215 07:16:57.544048 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.544210 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.544789 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.545045 140411286930816 create_pretraining_data.py:151] tokens: [CLS] अम ##ज ##ॉन [MASK] ##म वीडियो क कटट स मकाबला करन की दसी तयारी ह [MASK] [MASK] ##स [MASK] [MASK] की नई वब सीरीज - [MASK] ##ोक । [MASK] [MASK] ह [MASK] क ली ##ड एकटरस गल ##शन दव ##या और जिम सर ##भ स इस सीरीज [MASK] बार म । सलमान खान क बहन ##ो ##ई [MASK] ‘ लव ##या [MASK] ’ एकटर आयष शरमा न [MASK] 33 ##वा बरथड सलिबर ##ट किया [MASK] उनका बरथड काफी सप ##शल [MASK] स मनाया गया । इस पारटी म खान परिवार [MASK] लकर बॉलीवड क [SEP] परतीक पाड त ##ख ##तपर क बहत ही [MASK] कारयकरता मान जात ह और य विधायक परत ##या ##शी स पहल एक [MASK] ही भाजपा क क वरिषठ कारयकरता भी ह [SEP]\r\n",
      "I0215 07:16:57.545304 140411286930816 create_pretraining_data.py:161] input_ids: 2 570 237 1107 3 211 980 97 3161 127 6147 397 316 3286 4844 128 3 3 238 3 3 316 847 1008 1775 17 3 1352 136 3 3 128 3 97 714 220 4904 1022 448 729 313 322 2490 387 225 127 329 1775 3 460 120 136 4772 744 97 2071 228 216 3 149 2746 313 3 150 2359 2245 761 115 3 2454 355 4992 3537 236 393 3 1532 4992 770 641 1318 3 127 6092 413 136 329 1093 120 744 1017 3 768 2254 97 4 4122 1770 111 254 4386 97 669 381 3 3183 671 1473 128 322 121 1708 1012 313 836 127 635 338 3 381 1211 97 97 2218 3183 348 128 4\r\n",
      "I0215 07:16:57.545520 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.545743 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.545885 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 4 14 16 17 19 20 26 29 30 32 48 58 62 68 75 81 91 104 118 0\r\n",
      "I0215 07:16:57.546036 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 3386 4844 1641 253 730 286 347 5980 1689 1775 97 322 487 783 136 2106 127 6056 669 0\r\n",
      "I0215 07:16:57.546246 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.546384 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.546936 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.547209 140411286930816 create_pretraining_data.py:151] tokens: [CLS] 22 . एक परतियोगी परसकार म एक अभयरथी परतयक सही उततर क लिए [MASK] [MASK] परापत होत ह तथा परतयक गलत [MASK] क [MASK] 1 अक घट [MASK] ह [MASK] यदि वह सभी 75 परशन [MASK] ह तथा 125 अक [MASK] करता ह तो सही [MASK] गय परशन ##ो की सखया ह ? [SEP] [MASK] गए इस कदम को वी ##आईपी ससकति को खतम [MASK] की मह [MASK] स [MASK] ##कर दखा [MASK] रहा ह । इलाहाबाद ##ः योगी सरकार सरकषा क चाह लाख दा ##व कर ल , लकिन उसकी जमीन ##ी हकीकत कछ और ही [MASK] । परदश म [MASK] जनता तो छोड ##िए बीजपी क कारयकरता , नता भी सरकषित नही ह । इसी कडी म ताजा मामला इलाहाबाद का [MASK] । [MASK] [SEP]\r\n",
      "I0215 07:16:57.547439 140411286930816 create_pretraining_data.py:161] input_ids: 2 1117 18 338 3949 2860 120 338 6469 3146 1658 1229 97 350 3 3 1046 1322 128 822 3146 2097 3 97 3 21 1236 821 3 128 3 849 555 642 2281 3850 3 128 822 2835 1236 3 814 128 377 1658 3 1055 3850 228 316 1630 128 35 4 3 638 329 4003 317 531 4765 4755 317 2302 3 316 438 3 127 3 340 1962 3 542 128 136 6031 293 1484 498 1533 97 523 2085 2314 226 315 123 16 509 962 4975 207 6443 494 322 381 3 136 1002 120 3 1617 377 1387 331 1435 97 3183 16 1242 348 2368 357 128 136 1412 3261 120 2242 1870 6031 319 3 136 3 4\r\n",
      "I0215 07:16:57.547643 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.547842 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.547983 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 5 14 15 22 24 28 30 36 41 46 55 65 68 70 73 98 102 124 126 0\r\n",
      "I0215 07:16:57.548185 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 1371 24 1236 1229 350 318 69 814 1046 1285 350 397 561 5684 351 128 1216 128 746 0\r\n",
      "I0215 07:16:57.548345 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.548484 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.549037 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.549325 140411286930816 create_pretraining_data.py:151] tokens: [CLS] पति की मौत [MASK] वरष ##ीय हसी रानी सान ##याल पर इस कदर [MASK] ##वार [MASK] ##ा कि वो पति की लाश क बगल म चार दिन तक [MASK] रही इस बार [MASK] [MASK] पलिस न बताया कि पड ##ोस ##ियो का धयान दरवाज पर फक जान वाल अखबार ##ो ##पपा [MASK] [MASK] दिनो स ##बाजी किसी न नही उठा [MASK] थ । [SEP] अगर आप एक साथ कई [MASK] म काम करन क बार [MASK] [MASK] रह ह तो उन सभी भाषा पक को डाउनलोड कर जिनम आपको काम करन की जररत ह और फिर जब भी [MASK] इसकी [MASK] हो [MASK] उस विशष भाषा पक म कीबोरड चरण पर जाए । यह कीबोरड ऐप क अदर बहत आसानी स [MASK] जा सकता ह [MASK] [SEP]\r\n",
      "I0215 07:16:57.549551 140411286930816 create_pretraining_data.py:161] input_ids: 2 1906 316 1976 3 1071 2155 4380 2985 4366 1013 312 329 1571 3 392 3 209 326 777 1906 316 4661 97 2320 120 845 652 468 3 567 329 460 3 3 982 115 1005 326 848 3353 405 319 2316 4590 312 4312 431 462 4801 228 3845 3 3 1545 127 1861 536 115 357 2869 3 112 136 4 552 353 338 386 704 3 120 688 397 97 460 3 3 379 128 377 370 642 1143 1184 317 2109 315 4013 681 688 397 316 2101 128 322 712 504 348 3 1323 3 328 3 382 1479 1143 1184 120 2352 3721 312 816 136 416 2352 1719 97 5193 669 2228 127 3 351 689 128 3 4\r\n",
      "I0215 07:16:57.549774 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.549974 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.550153 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 4 14 16 29 33 34 51 52 53 56 61 70 76 77 99 101 103 122 126 0\r\n",
      "I0215 07:16:57.550305 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 3652 4714 5237 3712 120 2116 312 2498 704 4801 222 3539 120 1970 681 2267 377 393 136 0\r\n",
      "I0215 07:16:57.550460 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.550595 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.551171 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.551430 140411286930816 create_pretraining_data.py:151] tokens: [CLS] अपन टविटर अकाउट पर यह सची साझा [MASK] हए परधानमतरी [MASK] मोदी को [MASK] [MASK] और कहा कि सरवकषण क नती ##ज दरशा ##त ह [MASK] पशकश परधानमतरी [MASK] [MASK] नततव म सही दिशा म कित बढ रहा ह [MASK] एक अनय टवीट म उनहोन कहा , ' पिछल कछ वरषो म लोगो का [MASK] सरकारो और नताओ स उठ गया । लकिन मोदी जी क नततव और [MASK] सरकार की नीति ##यो क कारण यह परमा एक बार फिर कायम हआ ह , जो लोक ##ततर [MASK] आधार ह । परधानमतरी क नततव [MASK] हर [MASK] [SEP] इसलिए , आखिरकार उस पछा गया , \" कया आप अपन पिता [MASK] पर क चरणो [MASK] पालन करग जसा वह अन ##रो ##ध करता ह ? \" [SEP]\r\n",
      "I0215 07:16:57.551672 140411286930816 create_pretraining_data.py:161] input_ids: 2 369 2917 3690 312 416 3122 3055 3 624 1379 3 991 317 3 3 322 426 326 2423 97 4300 237 3908 208 128 3 5876 1379 3 3 2730 120 1658 1050 120 1843 767 542 128 3 338 885 6415 120 541 426 16 11 2105 494 4773 120 663 319 3 5721 322 2016 127 1080 413 136 509 991 453 97 2730 322 3 498 316 3101 337 97 963 416 5546 338 460 712 5564 769 128 16 441 1425 2156 3 911 128 136 1379 97 2730 3 623 3 4 1253 16 6367 382 2632 413 16 6 682 353 369 1099 3 312 97 6295 3 1908 1320 1503 555 411 412 232 814 128 35 6 4\r\n",
      "I0215 07:16:57.551877 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.552104 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.552285 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 8 11 14 15 26 27 29 30 36 40 54 55 69 77 88 95 97 111 115 0\r\n",
      "I0215 07:16:57.552438 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 540 1872 3217 512 326 659 991 97 1053 136 319 2269 990 2269 319 120 979 97 319 0\r\n",
      "I0215 07:16:57.552593 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.552730 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.553305 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.553587 140411286930816 create_pretraining_data.py:151] tokens: [CLS] गराह ##क अम ##जन इडिया की वबसाइट स एचडीएफसी बक [MASK] करडिट या डब ##िड [MASK] इसतमाल करक य समारटफोन खरीद ##त ह [MASK] उनह 2000 khilji [MASK] इसट ##ट डिसकाउट मिलगा । य ऑफर ई ##एम ##आई और बक [MASK] करडिट व डबिट कारड दोनो यजरस क लिए उपलबध ह । वनपलस 6 समारटफोन क 64 जीबी वरिएट की कीमत 34 [MASK] 999 रपए ह [SEP] भगवान [MASK] ##वय ##ास [MASK] रच ##ित अ ##ठा ##र ##ह पराण ##ो म स एक ' अगन ##ि पराण [MASK] म [MASK] ##िद ##व दवारा मह ##रष ##ि [MASK] को दिय गय विभिनन उपदश ह । इसी [MASK] अतर ##गत इस पाप ##नाश ##क सत ##ोत ##र क [MASK] [MASK] महातमा पष [MASK] [MASK] ह [ … ] [SEP]\r\n",
      "I0215 07:16:57.553819 140411286930816 create_pretraining_data.py:161] input_ids: 2 3538 212 570 590 1718 316 1378 127 4099 898 3 2785 545 1901 3010 3 1147 1014 121 1210 4868 208 128 3 456 5919 2249 3 5581 236 2264 3463 136 121 2952 87 942 1036 322 898 3 2785 124 3580 1069 750 4933 97 350 1366 128 136 2420 26 1210 97 2924 2378 3459 316 1753 4155 3 5116 2220 128 4 2897 3 1593 471 3 4341 342 84 1930 210 230 5765 228 120 127 338 11 3252 221 5765 3 120 3 780 226 674 438 3005 221 3 317 3738 1055 1877 5746 128 136 1412 3 1336 1131 329 3430 2786 212 778 2037 210 97 3 3 6033 5302 3 3 128 37 154 39 4\r\n",
      "I0215 07:16:57.554032 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.554259 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.554403 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 11 16 24 27 28 41 46 63 69 72 88 90 97 106 110 117 118 121 122 0\r\n",
      "I0215 07:16:57.554548 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 319 1069 16 2220 319 97 750 16 2749 674 11 3252 4800 97 3430 460 120 340 1618 0\r\n",
      "I0215 07:16:57.554702 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.554839 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.555438 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.555700 140411286930816 create_pretraining_data.py:151] tokens: [CLS] ऐस कई रिपोरट सामन आए ह जिनम [MASK] क माता - [MASK] [MASK] समलग ##िग [MASK] दर करन क लिए भाई या कज [MASK] स उनका जिसम ##ानी रिशता कायम करवा रह ह । [MASK] म एक महिला न अपन ही बट का रप [MASK] ह । आरोपी मा का कहना ह कि उसन अपन बटी की समलग ##िकता दर करन क लिए [SEP] म अगर राशि शनय हो गई ह तो भी बक [MASK] खाता भम [MASK] कर सकत [MASK] । साथ ही [MASK] ##िनि ##म ##म अम [MASK] ##उट क नाम पर किसी तरह का जरम ##ाना लगान का अधिकार भी [MASK] पास नही ह । बक क खात को दोबारा [MASK] करान क लिए बक आपस कोई [MASK] दरा ल सकता ह । [SEP]\r\n",
      "I0215 07:16:57.555927 140411286930816 create_pretraining_data.py:161] input_ids: 2 844 704 1712 1109 1174 128 4013 3 97 5757 17 3 3 4517 433 3 442 397 97 350 1612 545 3259 3 127 1532 1113 477 4954 5564 2784 379 128 136 3 120 338 919 115 369 381 1084 319 516 3 128 136 1983 535 319 2083 128 326 977 369 2319 316 4517 2354 442 397 97 350 4 120 552 1682 5367 328 544 128 377 348 898 3 2102 1912 3 315 564 3 136 386 381 3 5583 211 211 570 3 1319 97 705 312 536 738 319 2715 497 3096 319 1546 348 3 884 357 128 136 898 97 2482 317 5705 3 3853 97 350 898 5612 527 3 5283 123 689 128 136 4\r\n",
      "I0215 07:16:57.556148 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.556349 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.556489 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 8 12 13 16 24 33 35 45 61 75 77 78 81 85 90 104 114 121 122 0\r\n",
      "I0215 07:16:57.556639 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 5885 1099 990 318 213 128 3193 393 397 1142 581 357 128 120 209 1003 2306 1650 357 0\r\n",
      "I0215 07:16:57.556808 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.556945 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:57.557516 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.557954 140411286930816 create_pretraining_data.py:151] tokens: [CLS] [MASK] post [MASK] ##re ##vious दस कारण परयटक पयार सपिन रिवा ##इट [MASK] 9 . 0 [MASK] | सपिन रिवा ##इट ##र 9 . 0 स [MASK] पाच अदभत चीज सीख सकत ह [MASK] [SEP] [MASK] ##िका रही [MASK] बाद म अन ##क फिलमो म व दिल ##ी [MASK] कमार राज ##कप ##र और दव आनद [MASK] मा की भमिकाओ म आई । लीला चिटनिस न अभिनय करन क अलावा फिलम आज की बात 195 ##5 का निरमाण और निरदशन भी किया । हिद ##सतान [MASK] सिनमा म आन वाली [MASK] [MASK] ##तरियो म दरग ##ा खो ##ट क बाद लीला चिटनिस का नाम श [MASK] पर ह । अपन जीवन म उपाधयाय पति गज ठ ##न यश ##वत चिटनिस स तला [MASK] क बाद चार [SEP]\r\n",
      "I0215 07:16:57.558206 140411286930816 create_pretraining_data.py:161] input_ids: 2 3 1710 3 547 4880 914 963 6136 2019 5831 5347 1353 3 29 18 20 3 69 5831 5347 1353 210 29 18 20 127 3 1771 6104 2308 2587 564 128 3 4 3 628 567 3 432 120 411 212 2393 120 124 660 207 3 469 436 5401 210 322 729 4749 3 535 316 6184 120 1052 136 2605 2441 115 2637 397 97 1208 650 502 316 579 3114 288 319 2078 322 5736 348 393 136 1585 3079 3 967 120 910 1070 3 3 4531 120 3446 209 1806 236 97 432 2605 2441 319 705 125 3 312 128 136 369 830 120 3188 1906 1240 107 213 4334 1159 2441 127 2312 3 97 432 845 4\r\n",
      "I0215 07:16:57.558412 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.558607 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.558745 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 1 3 8 13 17 27 34 36 39 49 54 57 86 91 92 106 113 116 123 0\r\n",
      "I0215 07:16:57.558890 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 5723 265 6136 210 136 353 136 215 377 259 322 316 207 5934 1619 5809 348 314 212 0\r\n",
      "I0215 07:16:57.559045 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.559207 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.559751 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.560006 140411286930816 create_pretraining_data.py:151] tokens: [CLS] हर 1 जापानी यन म आजरबाइजान [MASK] की [MASK] विनिमय [MASK] दिखा ##ता [MASK] । 1 [MASK] [MASK] म जापानी यन [MASK] क लिए गराफ महम उल [MASK] कर । उपर वाल गराफ म आजरबाइजान मनात और जापानी यन क बीच की विनिमय दर [MASK] ह । जापानी यन और अनय मदरा की ऐतिहासिक विनिमय दर गराफ [MASK] लिए नीच वाली सची स [SEP] हमशा स एक अन ##चा ##ही सी टककर रही ह और अब य टककर एक बार नही दो दो बार होन जा रही ह । कारण ह [MASK] आन वाली फिलम । खबरो की मान तो अजय दवगन न बो [MASK] कपर क साथ [MASK] सपोरटस बायोपिक क लिए हाथ [MASK] ##या ह जिस तव ##र क डायरकटर अमित [MASK] डायर ##कट [SEP]\r\n",
      "I0215 07:16:57.560269 140411286930816 create_pretraining_data.py:161] input_ids: 2 623 21 3182 1654 120 2922 3 316 3 2880 3 2239 318 3 136 21 3 3 120 3182 1654 3 97 350 2415 2581 1642 3 315 136 4235 462 2415 120 2922 3568 322 3182 1654 97 933 316 2880 442 3 128 136 3182 1654 322 885 2851 316 2114 2880 442 2415 3 350 1778 1070 3122 127 4 1768 127 338 411 614 330 486 2889 567 128 322 496 121 2889 338 460 357 479 479 460 558 351 567 128 136 963 128 3 910 1070 650 136 2255 316 671 377 1717 2853 115 1127 3 1892 97 386 3 3642 3209 97 350 1549 3 313 128 472 4282 210 97 6331 3478 3 4281 958 4\r\n",
      "I0215 07:16:57.560474 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.560672 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.560813 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 7 9 11 14 17 18 22 26 28 45 55 58 92 99 105 108 109 115 124 0\r\n",
      "I0215 07:16:57.560964 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 3568 2114 442 128 2922 3568 1776 317 538 5018 2880 97 990 671 403 386 338 1728 761 0\r\n",
      "I0215 07:16:57.561142 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.561281 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.561967 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.562250 140411286930816 create_pretraining_data.py:151] tokens: [CLS] 4 . अगर आप कोई भी [MASK] पक बनान क [MASK] नही ह तो कीबोरड ऐप म जाकर ऑपशन स [MASK] विश ##स भाषा पक को चन और उस [MASK] [MASK] कर ##द । [SEP] news & g ##ad ##g ##et review ##s , f ##oll ##ow [MASK] ##iz ##b ##ot on twitter [MASK] facebook , youtube and als ##o subscribe to [MASK] not ##if [MASK] . 1 [MASK] अपन फोन की सटिग खोलकर language and in ##p ##ut [MASK] जाए लकर इसक बाद डि [MASK] ##ॉल ##ट कीबोरड ऐप क पीछ दिए गियर आइकन पर टप करक [MASK] [MASK] ##ए चन । अब आप available language लिसट म अपन [MASK] पर डाउनलोड की गई सभी भाषाओ को दख ##ग । जगह [MASK] लिसट स अपनी [SEP]\r\n",
      "I0215 07:16:57.562480 140411286930816 create_pretraining_data.py:161] input_ids: 2 24 18 552 353 527 348 3 1184 1205 97 3 357 128 377 2352 1719 120 2800 6405 127 3 605 238 1143 1184 317 925 322 382 3 3 315 214 136 4 543 10 48 444 241 604 5657 234 16 47 2406 619 3 4398 248 813 656 6215 3 2654 16 2914 859 5938 251 2910 787 3 1563 3801 3 18 21 3 369 1649 316 4677 6061 5039 859 490 265 572 3 816 768 616 432 2965 3 853 236 2352 1719 97 2844 1342 2959 5982 312 1900 1014 3 3 222 925 136 496 353 5033 5039 3770 120 369 3 312 2109 316 544 642 3539 317 525 217 136 1481 3 3770 127 578 4\r\n",
      "I0215 07:16:57.562685 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.562895 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.563035 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 7 11 21 30 31 48 54 63 66 69 80 81 82 86 99 100 111 122 123 0\r\n",
      "I0215 07:16:57.563211 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 1143 4886 382 1504 1134 48 16 3673 4648 18 312 816 136 223 5845 1143 3214 1143 316 0\r\n",
      "I0215 07:16:57.563368 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.563505 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 0\r\n",
      "I0215 07:16:57.564075 140411286930816 create_pretraining_data.py:149] *** Example ***\r\n",
      "I0215 07:16:57.564342 140411286930816 create_pretraining_data.py:151] tokens: [CLS] . एक अभयरथी को उतत ##ीर ##ण होन क लिए 35 % अक चाहिए | यदि वह 96 अक परापत [MASK] ह तथा 16 अको स ##ware ##तीरण रहता [MASK] तो अधिक [MASK] अक ह ? [MASK] . a को एक सखया को 25 स गणा करन को कहा गया था [MASK] उसन [MASK] स गणा सवसथ जिसक फलस ##वरप [MASK] उततर स 32 ##4 जयादा सखया [MASK] हई | गणा किय जान वाली सखया [SEP] परसा कोल बलॉक जनसनवाई म अडानी [MASK] धनबल का दर ##प ##योग : कमपनी की रिपोरट फरजी भम निकला ##मक और झ ##टी – [MASK] बॉस ##कट परसा कोल [MASK] जनसनवाई म अडानी दवारा [MASK] का दर ##प ##योग : कमपनी की रिपोरट फरजी भम ##ात ##मक और झ ##टी [SEP]\r\n",
      "I0215 07:16:57.564571 140411286930816 create_pretraining_data.py:161] input_ids: 2 18 338 6469 317 2223 1245 243 558 97 350 2455 9 1236 1004 69 849 555 3653 1236 1046 3 128 822 949 2695 127 4459 5616 1850 3 377 697 3 1236 128 35 3 18 42 317 338 1630 317 1118 127 6020 397 317 426 413 402 3 977 3 127 6020 5799 1690 4317 4576 3 1229 127 3222 278 891 1630 3 1009 69 6020 2564 431 1070 1630 4 5547 3395 5790 6464 120 6262 3 6090 319 442 259 611 30 4542 316 1712 3981 1912 3975 2518 322 104 419 147 3 3541 958 5547 3395 3 6464 120 6262 674 3 319 442 259 611 30 4542 316 1712 3981 1912 447 2518 322 104 419 4\r\n",
      "I0215 07:16:57.564774 140411286930816 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.564971 140411286930816 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      "I0215 07:16:57.565134 140411286930816 create_pretraining_data.py:161] masked_lm_positions: 1 21 27 30 33 37 49 52 54 57 61 68 77 83 95 101 106 111 113 0\r\n",
      "I0215 07:16:57.565288 140411286930816 create_pretraining_data.py:161] masked_lm_ids: 18 814 3080 128 625 27 426 69 4158 393 1658 1046 5547 674 447 3205 5790 6090 442 0\r\n",
      "I0215 07:16:57.565444 140411286930816 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\r\n",
      "I0215 07:16:57.565594 140411286930816 create_pretraining_data.py:161] next_sentence_labels: 1\r\n",
      "I0215 07:16:58.369853 140411286930816 create_pretraining_data.py:166] Wrote 2533 total instances\r\n"
     ]
    }
   ],
   "source": [
    "!python create_pretraining_data.py \\\n",
    "    --input_file=/kaggle/input/hindi-oscar-corpus/hi_dedup_1000.txt \\\n",
    "    --output_file=/kaggle/working/tf_examples.tfrecord \\\n",
    "    --vocab_file=/kaggle/working/hindi-vocab.txt \\\n",
    "    --do_lower_case=True \\\n",
    "    --max_seq_length=128 \\\n",
    "    --max_predictions_per_seq=20 \\\n",
    "    --masked_lm_prob=0.15 \\\n",
    "    --random_seed=42 \\\n",
    "    --dupe_factor=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0215 07:17:07.396560 140162862937472 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n",
      "\r\n",
      "W0215 07:17:07.396826 140162862937472 module_wrapper.py:139] From run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n",
      "\r\n",
      "W0215 07:17:07.397039 140162862937472 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "W0215 07:17:07.400176 140162862937472 module_wrapper.py:139] From run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n",
      "\r\n",
      "2020-02-15 07:17:19.469474: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\n",
      "W0215 07:17:19.531876 140162862937472 module_wrapper.py:139] From run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\r\n",
      "\r\n",
      "W0215 07:17:19.586900 140162862937472 module_wrapper.py:139] From run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n",
      "\r\n",
      "I0215 07:17:19.587259 140162862937472 run_pretraining.py:420] *** Input Files ***\r\n",
      "I0215 07:17:19.587421 140162862937472 run_pretraining.py:422]   gs://tf-lang-model/tf_examples.tfrecord\r\n",
      "W0215 07:17:19.587645 140162862937472 lazy_loader.py:50] \r\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\n",
      "For more information, please see:\r\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n",
      "  * https://github.com/tensorflow/addons\r\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\r\n",
      "If you depend on functionality not listed there, please file an issue.\r\n",
      "\r\n",
      "W0215 07:17:21.764867 140162862937472 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f79c0d69158>) includes params argument, but params are not passed to Estimator.\r\n",
      "I0215 07:17:21.766565 140162862937472 estimator.py:212] Using config: {'_model_dir': 'gs://tf-lang-model/model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n",
      "cluster_def {\r\n",
      "  job {\r\n",
      "    name: \"worker\"\r\n",
      "    tasks {\r\n",
      "      key: 0\r\n",
      "      value: \"10.0.0.2:8470\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "isolate_session_state: true\r\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f79c0d2f860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.0.0.2:8470', '_evaluation_master': 'grpc://10.0.0.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f79d7066ba8>}\r\n",
      "I0215 07:17:21.767647 140162862937472 tpu_context.py:220] _TPUContext: eval_on_tpu True\r\n",
      "I0215 07:17:21.768500 140162862937472 run_pretraining.py:459] ***** Running training *****\r\n",
      "I0215 07:17:21.768646 140162862937472 run_pretraining.py:460]   Batch size = 32\r\n",
      "I0215 07:17:22.332732 140162862937472 estimator.py:363] Skipping training since max_steps has already saved.\r\n",
      "I0215 07:17:22.333201 140162862937472 error_handling.py:101] training_loop marked as finished\r\n",
      "I0215 07:17:22.333485 140162862937472 run_pretraining.py:469] ***** Running evaluation *****\r\n",
      "I0215 07:17:22.333625 140162862937472 run_pretraining.py:470]   Batch size = 8\r\n",
      "I0215 07:17:22.333955 140162862937472 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.0.0.2:8470) for TPU system metadata.\r\n",
      "2020-02-15 07:17:22.335075: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\n",
      "I0215 07:17:22.340425 140162862937472 tpu_system_metadata.py:148] Found TPU system:\r\n",
      "I0215 07:17:22.340743 140162862937472 tpu_system_metadata.py:149] *** Num TPU Cores: 8\r\n",
      "I0215 07:17:22.340982 140162862937472 tpu_system_metadata.py:150] *** Num TPU Workers: 1\r\n",
      "I0215 07:17:22.341193 140162862937472 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\r\n",
      "I0215 07:17:22.341361 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10671751639536929526)\r\n",
      "I0215 07:17:22.342356 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14979538238226413203)\r\n",
      "I0215 07:17:22.342476 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1342462725291052991)\r\n",
      "I0215 07:17:22.342579 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 14467497591388249788)\r\n",
      "I0215 07:17:22.342676 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12773847690205870471)\r\n",
      "I0215 07:17:22.342776 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16678045150434640113)\r\n",
      "I0215 07:17:22.342869 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1999586173063525537)\r\n",
      "I0215 07:17:22.342969 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14632199629634247679)\r\n",
      "I0215 07:17:22.343083 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8405725506447244064)\r\n",
      "I0215 07:17:22.343198 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 10792116100497442753)\r\n",
      "I0215 07:17:22.343295 140162862937472 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7838129423417063256)\r\n",
      "W0215 07:17:22.452216 140162862937472 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "If using Keras pass *_constraint arguments to layers.\r\n",
      "I0215 07:17:22.455043 140162862937472 estimator.py:1148] Calling model_fn.\r\n",
      "W0215 07:17:22.455967 140162862937472 module_wrapper.py:139] From run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\r\n",
      "\r\n",
      "W0215 07:17:22.487633 140162862937472 deprecation.py:323] From run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\r\n",
      "W0215 07:17:22.487957 140162862937472 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\r\n",
      "W0215 07:17:22.602912 140162862937472 module_wrapper.py:139] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\r\n",
      "\r\n",
      "W0215 07:17:22.796657 140162862937472 deprecation.py:323] From run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use `tf.cast` instead.\r\n",
      "I0215 07:17:22.838558 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.841133 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.843601 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.846114 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.848737 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.851358 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.854502 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "I0215 07:17:22.857357 140162862937472 tpu_estimator.py:1201] Found small feature: next_sentence_labels [1, 1]\r\n",
      "2020-02-15 07:17:22.881256: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\r\n",
      "2020-02-15 07:17:22.881309: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2020-02-15 07:17:22.881349: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (687db16296c7): /proc/driver/nvidia/version does not exist\r\n",
      "I0215 07:17:22.906086 140162862937472 run_pretraining.py:117] *** Features ***\r\n",
      "I0215 07:17:22.906407 140162862937472 run_pretraining.py:119]   name = input_ids, shape = (1, 128)\r\n",
      "I0215 07:17:22.906589 140162862937472 run_pretraining.py:119]   name = input_mask, shape = (1, 128)\r\n",
      "I0215 07:17:22.906746 140162862937472 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\r\n",
      "I0215 07:17:22.906883 140162862937472 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\r\n",
      "I0215 07:17:22.907047 140162862937472 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\r\n",
      "I0215 07:17:22.907205 140162862937472 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\r\n",
      "I0215 07:17:22.907346 140162862937472 run_pretraining.py:119]   name = segment_ids, shape = (1, 128)\r\n",
      "W0215 07:17:22.907739 140162862937472 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n",
      "\r\n",
      "W0215 07:17:22.910554 140162862937472 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n",
      "\r\n",
      "W0215 07:17:22.952567 140162862937472 module_wrapper.py:139] From /kaggle/input/bertsrc/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\r\n",
      "\r\n",
      "W0215 07:17:23.016850 140162862937472 deprecation.py:323] From /kaggle/input/bertsrc/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use keras.layers.Dense instead.\r\n",
      "W0215 07:17:23.019045 140162862937472 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use `layer.__call__` method instead.\r\n",
      "W0215 07:17:25.925124 140162862937472 module_wrapper.py:139] From run_pretraining.py:150: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n",
      "\r\n",
      "I0215 07:17:25.925469 140162862937472 run_pretraining.py:167] **** Trainable Variables ****\r\n",
      "I0215 07:17:25.925595 140162862937472 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768)\r\n",
      "I0215 07:17:25.925740 140162862937472 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\r\n",
      "I0215 07:17:25.925861 140162862937472 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\r\n",
      "I0215 07:17:25.926019 140162862937472 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.926185 140162862937472 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.926298 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.926418 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.926527 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.926656 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.926763 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.926874 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.926980 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.927115 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.927228 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.927334 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.927443 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.927555 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.927663 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.927775 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.927882 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.927986 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.928110 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.928225 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.928331 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.928443 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.928549 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.928660 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.928767 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.928880 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.928986 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.929115 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.929228 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.929347 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.929455 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.929568 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.929675 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.929781 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.929890 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.930003 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.930146 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.930265 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.930375 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.930492 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.930601 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.930715 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.930824 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.930932 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.931039 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.931179 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.931289 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.931403 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.931511 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.931618 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.931725 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.931838 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.931945 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.932077 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.932193 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.932308 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.932418 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.932530 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.932639 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.932745 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.932854 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.932969 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.933094 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.933253 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.933365 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.933470 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.933574 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.933686 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.933800 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.933912 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.934018 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.934153 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.934260 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.934382 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.934488 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.934593 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.934696 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.934808 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.934911 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.935023 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.935151 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.935257 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.935361 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.935472 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.935581 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.935693 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.935799 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.935908 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.936014 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.936148 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.936254 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.936361 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.936467 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.936579 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.936686 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.936797 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.936904 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.937011 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.937139 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.937253 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.937369 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.937483 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.937589 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.937700 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.937805 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.937917 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.938026 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.938153 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.938260 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.938371 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.938481 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.938592 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.938696 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.938800 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.938904 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.939015 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.939141 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.939254 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.939359 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.939469 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.939574 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.939687 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.939792 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.939896 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.940001 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.940131 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.940238 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.940350 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.940454 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.940560 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.940665 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.940777 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.940891 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.941002 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.941128 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.941242 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.941365 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.941476 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.941580 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.941685 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.941809 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.941926 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.942034 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.942166 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.942273 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.942379 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.942484 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.942595 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.942700 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.942812 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.942917 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.943028 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.943157 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.943271 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.943376 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.943483 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.943587 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.943697 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.943803 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.943912 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.944020 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.944145 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.944252 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.944362 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.944481 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.944593 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.944700 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.944809 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.944915 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.945025 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.945153 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.945260 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.945365 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.945477 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.945583 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.945693 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.945798 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.945908 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.946013 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.946147 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.946265 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.946379 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.946543 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.946656 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.946760 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.946871 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.946976 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.947098 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.947207 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\r\n",
      "I0215 07:17:25.947319 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\r\n",
      "I0215 07:17:25.947423 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\r\n",
      "I0215 07:17:25.947534 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.947640 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.947745 140162862937472 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.947849 140162862937472 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.947963 140162862937472 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.948095 140162862937472 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\r\n",
      "I0215 07:17:25.948213 140162862937472 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\r\n",
      "I0215 07:17:25.948317 140162862937472 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\r\n",
      "I0215 07:17:25.948425 140162862937472 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\r\n",
      "I0215 07:17:25.948529 140162862937472 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30522,)\r\n",
      "I0215 07:17:25.948635 140162862937472 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\r\n",
      "I0215 07:17:25.948745 140162862937472 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\r\n",
      "W0215 07:17:25.964576 140162862937472 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Deprecated in favor of operator or tf.math.divide.\r\n",
      "W0215 07:17:26.389129 140162862937472 module_wrapper.py:139] From run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\r\n",
      "\r\n",
      "W0215 07:17:26.408415 140162862937472 module_wrapper.py:139] From run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\r\n",
      "\r\n",
      "I0215 07:17:26.609611 140162862937472 estimator.py:1150] Done calling model_fn.\r\n",
      "I0215 07:17:26.632464 140162862937472 evaluation.py:255] Starting evaluation at 2020-02-15T07:17:26Z\r\n",
      "I0215 07:17:26.632811 140162862937472 tpu_estimator.py:506] TPU job name worker\r\n",
      "W0215 07:17:26.784030 140162862937472 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\r\n",
      "I0215 07:17:27.227380 140162862937472 monitored_session.py:240] Graph was finalized.\r\n",
      "I0215 07:17:27.264560 140162862937472 saver.py:1284] Restoring parameters from gs://tf-lang-model/model/model.ckpt-20\r\n",
      "I0215 07:17:47.889324 140162862937472 session_manager.py:500] Running local_init_op.\r\n",
      "I0215 07:17:48.071941 140162862937472 session_manager.py:502] Done running local_init_op.\r\n",
      "W0215 07:17:48.235172 140162862937472 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:802: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\r\n",
      "I0215 07:17:48.324774 140162862937472 tpu_estimator.py:567] Init TPU system\r\n",
      "I0215 07:17:50.956387 140162862937472 tpu_estimator.py:576] Initialized TPU in 2 seconds\r\n",
      "I0215 07:17:50.957300 140160410838784 tpu_estimator.py:521] Starting infeed thread controller.\r\n",
      "I0215 07:17:50.957668 140160402446080 tpu_estimator.py:540] Starting outfeed thread controller.\r\n",
      "I0215 07:17:51.468606 140162862937472 util.py:98] Initialized dataset iterators in 0 seconds\r\n",
      "I0215 07:17:51.562662 140162862937472 tpu_estimator.py:600] Enqueue next (100) batch(es) of data to infeed.\r\n",
      "I0215 07:17:51.563873 140162862937472 tpu_estimator.py:604] Dequeue next (100) batch(es) of data from outfeed.\r\n",
      "I0215 07:17:56.884239 140160402446080 tpu_estimator.py:279] Outfeed finished for iteration (0, 0)\r\n",
      "I0215 07:17:59.383916 140162862937472 evaluation.py:167] Evaluation [100/100]\r\n",
      "I0215 07:17:59.384326 140162862937472 tpu_estimator.py:608] Stop infeed thread controller\r\n",
      "I0215 07:17:59.384499 140162862937472 tpu_estimator.py:434] Shutting down InfeedController thread.\r\n",
      "I0215 07:17:59.384760 140160410838784 tpu_estimator.py:429] InfeedController received shutdown signal, stopping.\r\n",
      "I0215 07:17:59.384925 140160410838784 tpu_estimator.py:537] Infeed thread finished, shutting down.\r\n",
      "I0215 07:17:59.385150 140162862937472 error_handling.py:101] infeed marked as finished\r\n",
      "I0215 07:17:59.385335 140162862937472 tpu_estimator.py:612] Stop output thread controller\r\n",
      "I0215 07:17:59.385477 140162862937472 tpu_estimator.py:434] Shutting down OutfeedController thread.\r\n",
      "I0215 07:18:00.422085 140160402446080 tpu_estimator.py:429] OutfeedController received shutdown signal, stopping.\r\n",
      "I0215 07:18:00.422405 140160402446080 tpu_estimator.py:551] Outfeed thread finished, shutting down.\r\n",
      "I0215 07:18:00.422705 140162862937472 error_handling.py:101] outfeed marked as finished\r\n",
      "I0215 07:18:00.422947 140162862937472 tpu_estimator.py:616] Shutdown TPU system.\r\n",
      "I0215 07:18:00.839760 140162862937472 evaluation.py:275] Finished evaluation at 2020-02-15-07:18:00\r\n",
      "I0215 07:18:00.840209 140162862937472 estimator.py:2049] Saving dict for global step 20: global_step = 20, loss = 9.97417, masked_lm_accuracy = 0.027236842, masked_lm_loss = 9.297666, next_sentence_accuracy = 0.5725, next_sentence_loss = 0.6918845\r\n",
      "I0215 07:18:02.772599 140162862937472 estimator.py:2109] Saving 'checkpoint_path' summary for global step 20: gs://tf-lang-model/model/model.ckpt-20\r\n",
      "I0215 07:18:03.185888 140162862937472 error_handling.py:101] evaluation_loop marked as finished\r\n",
      "I0215 07:18:03.186369 140162862937472 run_pretraining.py:483] ***** Eval results *****\r\n",
      "I0215 07:18:03.186524 140162862937472 run_pretraining.py:485]   global_step = 20\r\n",
      "I0215 07:18:03.186881 140162862937472 run_pretraining.py:485]   loss = 9.97417\r\n",
      "I0215 07:18:03.187048 140162862937472 run_pretraining.py:485]   masked_lm_accuracy = 0.027236842\r\n",
      "I0215 07:18:03.187204 140162862937472 run_pretraining.py:485]   masked_lm_loss = 9.297666\r\n",
      "I0215 07:18:03.187313 140162862937472 run_pretraining.py:485]   next_sentence_accuracy = 0.5725\r\n",
      "I0215 07:18:03.187422 140162862937472 run_pretraining.py:485]   next_sentence_loss = 0.6918845\r\n"
     ]
    }
   ],
   "source": [
    "!python run_pretraining.py \\\n",
    "    --input_file=gs://tf-lang-model/*.tfrecord \\\n",
    "    --output_dir=gs://tf-lang-model/model/ \\\n",
    "    --do_train=True \\\n",
    "    --do_eval=True \\\n",
    "    --bert_config_file=/kaggle/input/bert-base-uncased/config.json \\\n",
    "    --train_batch_size=32 \\\n",
    "    --max_seq_length=128 \\\n",
    "    --max_predictions_per_seq=20 \\\n",
    "    --num_train_steps=20 \\\n",
    "    --num_warmup_steps=10 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --use_tpu=True \\\n",
    "    --tpu_name=$TPU_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
